{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afabaecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T01:34:28.936184Z",
     "iopub.status.busy": "2022-01-05T01:34:28.935944Z",
     "iopub.status.idle": "2022-01-05T01:34:28.958183Z",
     "shell.execute_reply": "2022-01-05T01:34:28.957231Z",
     "shell.execute_reply.started": "2022-01-05T01:34:28.936157Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SQLContext \n",
    "import pandas as pd \n",
    "sqlc=SQLContext(sc)\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f2906",
   "metadata": {},
   "source": [
    "# 1. Duplicate Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Duplicate Transactions\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE duplicate_transactions (\n",
    "\ttransaction_id VARCHAR,\n",
    "\ttimestamp INTEGER,\n",
    "\tprice INTEGER,\n",
    "\tdepartment VARCHAR \n",
    "); \n",
    "\n",
    "\n",
    "COPY duplicate_transactions\n",
    "FROM '<Path to product_sql folder>/product_sql/duplicate_transactions/duplicate_transactions.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Medium\n",
    "Duration: 30 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "The duplicate_transactions table contains transaction_id, timestamp, price and department.\n",
    "\n",
    "Address these three questions:\n",
    "\n",
    "#1. How many duplicate records are there? For instance, if Row 1, and Row 2 and Row 3 contain\n",
    "the same values, then there are two duplicate records.\n",
    "\n",
    "#2. How many unique records have duplications?\n",
    "\n",
    "#3. Remove duplicate records, only preserving the unique records.\n",
    "\n",
    "#4. Which department has the highest duplicate records? Return the department\n",
    "name and count of duplicate records. Assume the possibility that multiple departments\n",
    "could have the same highest count.\n",
    "\n",
    "*/\n",
    "\n",
    "-- Solution #1\n",
    "\n",
    "-- Sum to get the total number of duplicate records\n",
    "SELECT SUM(number_of_dups) AS number_of_duplications\n",
    "FROM (\n",
    "\t-- Use GROUP BY to count repeating records per unique record\n",
    "\tSELECT transaction_id, timestamp, price, department, \n",
    "\t\tCOUNT(*) - 1 AS number_of_dups\n",
    "\tFROM duplicate_transactions\n",
    "\tGROUP BY transaction_id, timestamp, price, department \n",
    ") t;\n",
    "\n",
    "\n",
    "-- Solution #2\n",
    "\n",
    "-- Unique records with no duplications would have 0 in number_of_dups;\n",
    "-- Remove such records then count the rest which are the unique records\n",
    "-- with duplicate entries.\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "\t-- Use GROUP BY to count repeating records per unique record\n",
    "\tSELECT transaction_id, timestamp, price, department, \n",
    "\t\tCOUNT(*) - 1 AS number_of_dups\n",
    "\tFROM duplicate_transactions\n",
    "\tGROUP BY transaction_id, timestamp, price, department \n",
    ") t\n",
    "WHERE number_of_dups > 0;\n",
    "\n",
    "\n",
    "-- Solution #3\n",
    "\n",
    "-- Just apply DISTINCT to remove duplicate entries \n",
    "SELECT DISTINCT *\n",
    "FROM duplicate_transactions;\n",
    "\n",
    "\n",
    "-- Solution #4\n",
    "\n",
    "WITH CountDuplicates AS (\n",
    "\t-- Use GROUP BY to count repeating records per unique record\n",
    "\tSELECT transaction_id, timestamp, price, department, \n",
    "\t\tCOUNT(*) - 1 AS number_of_dups\n",
    "\tFROM duplicate_transactions\n",
    "\tGROUP BY transaction_id, timestamp, price, department \n",
    "),\n",
    "CountDuplicatesPerDept AS (\n",
    "\t-- Sum the duplicate entries per department\n",
    "\tSELECT\n",
    "\t\tdepartment,\n",
    "\t\tSUM(number_of_dups) AS number_of_dups\n",
    "\tFROM CountDuplicates\n",
    "\tGROUP BY department\n",
    "),\n",
    "RankByDups AS (\n",
    "\tSELECT\n",
    "\t\t-- Use DENSE_RANK to rank the department given the number of \n",
    "\t\t-- duplicate entries. DENSE_RANK considers repeated values.\n",
    "\t\tdepartment,\n",
    "\t\tnumber_of_dups,\n",
    "\t\tDENSE_RANK() OVER(ORDER BY number_of_dups DESC) AS dups_ranking\n",
    "\tFROM CountDuplicatesPerDept\n",
    ")\n",
    "-- Filter on dups_ranking = 1 as that indicates the department(s) with the \n",
    "-- highest number of duplications.\n",
    "SELECT department, number_of_dups  \n",
    "FROM RankByDups\n",
    "WHERE dups_ranking = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f12b6",
   "metadata": {},
   "source": [
    "# 2. Facebook connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE friends_connections (\n",
    "\tdate VARCHAR,\n",
    "\tuser_id FLOAT,\n",
    "\treceiver_id INTEGER,\n",
    "\taction VARCHAR \n",
    "); \n",
    "\n",
    "\n",
    "COPY friends_connections\n",
    "FROM '<Path to product_sql folder>/product_sql/friends_connections/friends_connections.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Medium\n",
    "Duration: 30 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "Facebook’s analytics team wants to understand how users stay connected among friends on their platform. \n",
    "The team believes that understanding patterns could help improve an algorithm that matches potential friends. \n",
    "Use the friends table below to address questions below. A user can perform the following sequence of actions: \n",
    "(1) request or receive, (2) connect, and (3) block.\n",
    "\n",
    "Address these two questions:\n",
    "\n",
    "#1. ​Return a list of users who blocked another user after connecting for at least 90 days. \n",
    "Show the user_id and receiver_id.\n",
    "\n",
    "#2. ​For each user, what is the proportion of each action? Note that the receiver_id can \n",
    "appear in multiple actions per user, only regard the latest status when calculating \n",
    "the distribution.\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "-- #1 Return a list of users who blocked another user after connecting for at least 90 days. \n",
    "-- \t Show user_id and receiver_id. \n",
    "\n",
    "-- Return a pair of users who once connected for at least 90 days then blocked.\n",
    "SELECT u.user_id, \n",
    "       o.receiver_id\n",
    "FROM\n",
    "(\n",
    "\t-- Before a user can block another user, the users must be connected. Create two\n",
    "\t-- sub-queries - one with blocked events and another with connected events.\n",
    "\t-- Use inner join on user_ids and receiver_ids.\n",
    "\t(SELECT * \n",
    "\t\tFROM connections\n",
    "\t\tWHERE action = 'Blocked') u\n",
    "\tJOIN\n",
    "\t(SELECT * \n",
    "\t\tFROM connections\n",
    "\t\tWHERE action = 'Connected') o\n",
    "\tON u.user_id = o.user_id AND u.receiver_id = o.receiver_id\n",
    ")\n",
    "WHERE (u.dates - o.dates) >= 90;\n",
    "\n",
    "-- #2 - For each user, what is the proportion of each action? Note that the receiver_id \n",
    "-- can appear in multiple actions per user, only regard the latest action performed \n",
    "-- when calculating the distribution.\n",
    "\n",
    "-- Assign event order by user_id and receiver_id to filter on the latest event\n",
    "WITH friendship_status AS (\n",
    "\tSELECT *,  \n",
    "\t\t   ROW_NUMBER() OVER(PARTITION BY user_id, receiver_id ORDER BY dates DESC) AS event_order\n",
    "\tFROM connections \n",
    "),  \n",
    "-- Filter on the latest event per user_id and receiver_id pair\n",
    "latest_friendship_status AS (\n",
    "\tSELECT *\n",
    "\tFROM friendship_status\n",
    "\tWHERE event_order = 1\n",
    "),\n",
    "-- Create a dummy variable column per action type\n",
    "status_dummy_variables AS (\n",
    "\tSELECT *,\n",
    "\t\t   CASE WHEN action = 'Sent' THEN 1 ELSE 0 END AS sent,\n",
    "\t\t   CASE WHEN action = 'Received' THEN 1 ELSE 0 END AS received,\n",
    "\t\t   CASE WHEN action = 'Connected' THEN 1 ELSE 0 END AS connected,\n",
    "\t\t   CASE WHEN action = 'Blocked' THEN 1 ELSE 0 END AS blocked\n",
    "\tFROM latest_friendship_status\n",
    "),\n",
    "-- For each action column, divide by event order to get proportion of action types per user.\n",
    "distribution AS (\n",
    "\tSELECT user_id,\n",
    "\t\t \tSUM(sent) / SUM(event_order) AS sent,\n",
    "\t\t \tSUM(received) / SUM(event_order) AS received,\n",
    "\t\t \tSUM(connected) / SUM(event_order) AS connected,\n",
    "\t\t \tSUM(blocked) / SUM(event_order) AS blocked\n",
    "\tFROM status_dummy_variables\n",
    "\tGROUP BY user_id\n",
    ")\n",
    "SELECT * FROM distribution;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cd853",
   "metadata": {},
   "source": [
    "# 3. Find Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78924a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Find Median\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE median_users (\n",
    "\tuser_id INTEGER,\n",
    "\tuser_creation_date DATE\n",
    "); \n",
    "\n",
    "CREATE TABLE median_transactions (\n",
    "\tuser_id INTEGER,\n",
    "\ttransaction_date DATE,\n",
    "\ttransaction_amount INTEGER\n",
    ");\n",
    "\n",
    "COPY median_users\n",
    "FROM '<Path to product_sql folder>/product_sql/find_median/median_users.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "COPY median_transactions\n",
    "FROM '<Path to product_sql folder>/product_sql/find_median/median_transactions.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Hard\n",
    "Duration: 15 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "There are two tables users and transactions. The median_users table contains the user_id \n",
    "and user_creation_date. The median_transactions table contains the user_id, transaction_date,\n",
    "and transaction_amount. A user can purchase as a visitor even before creating an account, \n",
    "and they user_id remains the same. \n",
    "\n",
    "#1. Among transactions that occurred on or after the date of sign-up, find the median and average\n",
    "per user.\n",
    "\n",
    "*/\n",
    "\n",
    "-- Solution\n",
    "WITH UserTransactions AS ( \n",
    "\t-- Join users and transactions table and filter on transactions that came\n",
    "\t-- after the sign-up\n",
    "\tSELECT \n",
    "\t\tmedian_users.user_id,\n",
    "\t\tuser_creation_date,\n",
    "\t\ttransaction_date,\n",
    "\t\ttransaction_amount\t\t\n",
    "\tFROM median_users \n",
    "\tJOIN median_transactions\n",
    "\t\tUSING (user_id)\n",
    "\tWHERE transaction_date >= user_creation_date\n",
    "), \n",
    "RowNumber AS (\n",
    "\t-- Create row_numbers in ascending and descending orders which will be useful for \n",
    "\t-- calculating the median. \n",
    "\tSELECT \n",
    "\t\tuser_id,\n",
    "\t\ttransaction_date,\n",
    "\t\ttransaction_amount,\n",
    "\t\trow_number() OVER(PARTITION BY user_id ORDER BY transaction_amount ASC) AS ascending,\n",
    "\t\trow_number() OVER(PARTITION BY user_id ORDER BY transaction_amount DESC) AS descending\n",
    "\tFROM UserTransactions\n",
    ")\n",
    "-- Calculating the average is straightforward as you can use the AVG() function. The median,\n",
    "-- however, can be somewhat tricky. The descending column serves as a reference point for the \n",
    "-- ascending column. When values in the ascending column are within the bounds of descending - 1\n",
    "-- and ascending + 1, preserve the values. Otherwise null. Averaging this column returns the median.\n",
    "SELECT user_id,\n",
    "\t   AVG(transaction_amount) AS average,\n",
    "\t   AVG(CASE \n",
    "\t   \t\tWHEN ascending BETWEEN descending - 1 AND descending + 1 \n",
    "\t   \t\tTHEN transaction_amount ELSE NULL \n",
    "\t   \t\tEND) AS median\n",
    "FROM RowNumber\n",
    "GROUP BY user_id; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d710b",
   "metadata": {},
   "source": [
    "# 4. Google Ads Spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Google Ads Spending\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE google_ads_spending (\n",
    "\tdate DATE,\n",
    "\tbusiness VARCHAR,\n",
    "\tspending FLOAT\n",
    "); \n",
    "\n",
    "\n",
    "COPY google_ads_spending\n",
    "FROM '<Path to product_sql folder>/product_sql/google_ads_spending/google_ads_spending.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Medium\n",
    "Duration: 30 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "A social network platform allows businesses to publish advertisements. The platform tracks \n",
    "daily advertisment spendings across business accounts. Currently, the platform\n",
    "doesn’t have a backend system to alert unsual spendings. Address the SQL questions below:\n",
    "\n",
    "Address these two questions:\n",
    "\n",
    "#1​.​ Compute the 30-day moving average of advertisement spending per business.\n",
    "\n",
    "#2​.​ Compute the 30-day moving standard deviation of advertisement spending per business.\n",
    "\n",
    "#3.​ The platform wants to track anomalous spendings. Create a new column called “outlier” \n",
    "which flags any spending that is above or below the two standard deviation from the mean. \n",
    "Use the moving average and standard deviation computed in previous steps.\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "-- #1 For each business, compute the moving average of the costs for the past thirty days.\n",
    "\n",
    "SELECT *,\n",
    "\t   -- Compute the moving average of spending\n",
    "\t   (SELECT AVG(s2.spending) FROM ads_spending s2 \n",
    "\t   \tWHERE (s1.date - s2.date) <= 30 AND\n",
    "\t   \t\t  (s1.date - s2.date) >= 0 AND\n",
    "\t   \t\t   s1.business = s2.business) AS moving_avg\n",
    "FROM ads_spending s1;\n",
    "\n",
    "-- #2 - For each business, compute the moving standard deviation of the costs for the past thirty days.\n",
    "\n",
    "SELECT *,\n",
    "\t   -- Compute the moving std of spending\n",
    "\t   (SELECT POWER(AVG(POWER(s3.spending - t1.moving_avg, 2)), 0.5) FROM ads_spending s3\n",
    "\t   \tWHERE (t1.date - s3.date) <= 30 AND\n",
    "\t   \t\t  (t1.date - s3.date) >= 0 AND\n",
    "\t   \t\t   t1.business = s3.business) AS moving_std\n",
    "FROM (\t   \t\t   \n",
    "\tSELECT *,\n",
    "\t\t   -- Compute the moving average of spending\n",
    "\t\t   (SELECT AVG(s2.spending) FROM ads_spending s2 \n",
    "\t\t   \tWHERE (s1.date - s2.date) <= 30 AND\n",
    "\t\t\t\t  (s1.date - s2.date) >= 0 AND\n",
    "\t\t   \t\t   s1.business = s2.business) AS moving_avg\n",
    "\tFROM ads_spending s1\n",
    ") t1;\n",
    "\n",
    "-- #3 - The platform wants to track anomalous spendings. Create a new column called “outlier” \n",
    "-- which flags any spending that is above or below the two standard deviation from the mean. \n",
    "-- Use the moving average and standard deviation computed in previous steps.\n",
    "\n",
    "SELECT *,\n",
    "       -- Append a new column called ‘outlier’ that flags outlier spendings\n",
    "\t   (CASE WHEN spending > moving_avg + 2 * moving_std THEN 1\n",
    "\t   \t\t WHEN spending < moving_avg - 2 * moving_std THEN 1\n",
    "\t   \t\t else 0\n",
    "\t   \tEND) AS outlier\n",
    "FROM (\t   \t\t \n",
    "\tSELECT *,\n",
    "           -- Compute the moving std of spending\n",
    "\t\t   (SELECT POWER(AVG(POWER(s3.spending - t1.moving_avg, 2)), 0.5) FROM ads_spending s3\n",
    "\t\t   \tWHERE (t1.date - s3.date) <= 30 AND\n",
    "\t\t   \t\t  (t1.date - s3.date) >= 0 AND\n",
    "\t\t   \t\t   t1.business = s3.business) AS moving_std\n",
    "\tFROM (\t   \t\t   \n",
    "\t\tSELECT *,\n",
    "         \t   -- Compute the moving average of spending\n",
    "\t\t\t   (SELECT AVG(s2.spending) FROM ads_spending s2 \n",
    "\t\t\t   \tWHERE (s1.date - s2.date) <= 30 AND\n",
    "\t\t\t\t\t  (s1.date - s2.date) >= 0 AND\n",
    "\t\t\t   \t\t   s1.business = s2.business) AS moving_avg\n",
    "\t\tFROM ads_spending s1\n",
    "\t) t1\n",
    ") t2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265aea6",
   "metadata": {},
   "source": [
    "# 5. LinkedIn job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Facebook Connections\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE linkedin_job_postings (\n",
    "\tdate VARCHAR,\n",
    "\tuser_id FLOAT,\n",
    "\treceiver_id INTEGER,\n",
    "\taction VARCHAR \n",
    "); \n",
    "\n",
    "\n",
    "COPY linkedin_job_postings\n",
    "FROM '<Path to product_sql folder>/product_sql/job_postings/linkedin_job_postings.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Hard\n",
    "Duration: 15 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "On LinkedIn, companies companies can advertise job openings. The data team wants to categorize job posts \n",
    "as either ​active, expired, ​or​ repeats​.\n",
    "\n",
    "> A post is active if it was published the past 30 days before 09/11/19 (current date). \n",
    "> A post is expired if it was published 30 days before 09/11/19.\n",
    "> A post is a repeat if it was expired then active again.\n",
    "\n",
    "Aaddress the following question: \n",
    "\n",
    "#1. Count the number of active and expired posts. Do not count repeats in the active-only or expired buckets.\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "-- #1. Count the number of active and expired posts. Do not count repeats in the active-only or expired buckets.\n",
    "\n",
    "-- Count the number of non-repeated active, expired and repeated posts.\n",
    "-- Do not include repeats in the active-only bucket.\n",
    "\n",
    "WITH post_snapshot AS (\n",
    "-- For each post snapshot, tag whether the snapshot is active or expired\n",
    "-- This label is required to remove repeats.\n",
    "SELECT post_id, advertiser_id, timestamp,\n",
    "\t   CASE WHEN ('2019-09-11' - timestamp) <= 30 THEN 1\n",
    "\t        ELSE 0\n",
    "\t   END AS snapshot_active,\n",
    "\t   CASE WHEN ('2019-09-11' - timestamp) > 30 THEN 1\n",
    "\t        ELSE 0\n",
    "\t   END AS snapshot_expired\n",
    "FROM job_postings\n",
    "),\n",
    "\tpost_status AS (\n",
    "\t-- Tag a post as active, expired, repeat given the logic: If the post\n",
    "\t-- contains 1 snapshot that's active and no snapshot expired, then it's\n",
    "\t-- active-only. If it contains no active and multiple expired snapshots,\n",
    "\t-- then it's expired. All other post instances are repeats. \n",
    "\tSELECT post_id, advertiser_id,\n",
    "\t\t   CASE WHEN current_active = 1 AND number_of_expired = 0 THEN 'Active'\n",
    "\t\t        WHEN current_active = 0 AND number_of_expired > 0 THEN 'Expired'\n",
    "\t\t        ELSE 'Repeat'\n",
    "\t\t    END AS status\n",
    "\tFROM (\n",
    "\t\t-- For each post, count the number of snapshots that were active\n",
    "\t\t-- and expired. This will determine whether the post is active-only,\n",
    "\t\t-- expired or repeat.\n",
    "\t\tSELECT post_id, advertiser_id,\n",
    "\t\t\t   SUM(snapshot_active) AS current_active,\n",
    "\t\t\t   SUM(snapshot_expired) AS number_of_expired\n",
    "\t\tFROM post_snapshot\n",
    "\t\tGROUP BY 1, 2\n",
    "\t) t\n",
    ")\n",
    "-- Using post_status, count the number of instances.\n",
    "SELECT status,\n",
    "\t   COUNT(*) AS counts\n",
    "FROM post_status\n",
    "GROUP BY 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa31c5a",
   "metadata": {},
   "source": [
    "# 6. Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d404f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Precision-Recall\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE precision_recall (\n",
    "\ttransaction_id VARCHAR,\n",
    "\tprobability FLOAT,\n",
    "\tlabels INTEGER,\n",
    "\tdepartment VARCHAR \n",
    "); \n",
    "\n",
    "\n",
    "COPY precision_recall\n",
    "FROM '<Path to product_sql folder>/product_sql/precision_recall/precision_recall.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Medium\n",
    "Duration: 15 Minutes\n",
    "    \n",
    "df =sqlc.sql(\"\"\"\n",
    "WITH Prediction AS (\n",
    "SELECT\n",
    "CASE WHEN probability > 0.7 THEN 1 ELSE 0 END AS prediction,\n",
    "labels\n",
    "FROM metro_ca_analyst_tmp.di_v1\n",
    "),\n",
    "CorrectPrediction AS (\n",
    "SELECT\n",
    "prediction,\n",
    "labels,\n",
    "CASE WHEN prediction = 1 AND labels = 1 THEN 1 ELSE 0 END AS correct_prediction\n",
    "FROM Prediction\n",
    ")\n",
    "SELECT\n",
    "SUM(correct_prediction) * 1.0 / SUM(prediction) AS precision,\n",
    "SUM(correct_prediction) * 1.0 / SUM(labels) AS recall\n",
    "FROM CorrectPrediction\n",
    "\"\"\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# df.write.saveAsTable('metro_ca_analyst_tmp.<>', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ca6ea",
   "metadata": {},
   "source": [
    "# 7. Revenue Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d346929",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Revenue Analytics\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE revenue_analytics (\n",
    "\tcompany VARCHAR,\n",
    "\tyear INTEGER,\n",
    "\trevenue FLOAT\n",
    "); \n",
    "\n",
    "\n",
    "COPY revenue_analytics\n",
    "FROM '<Path to product_sql folder>/product_sql/revenue_analytics/revenue_analytics.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Easy\n",
    "Duration: 15 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "Given the company revenue data below, address two questions:\n",
    "\n",
    "#1​. For each year, return the names of companies with the top 10th percentile \n",
    "revenue. Also return years and revenues.\n",
    "\n",
    "#2​. Return the names of companies that grew their YoY revenue by at \n",
    "least 5%, consecutively every year.\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "-- #1 For each year, return the names of companies with the top 10th percentile revenue. \n",
    "--    Also return years and revenues.\n",
    "\n",
    "SELECT company, year, revenue\n",
    "  FROM (\n",
    "  \t-- Within each year, rank companies based on their revenue in a descending order\n",
    "\tSELECT company, year, revenue,\n",
    "\t\t   ROW_NUMBER() OVER(PARTITION BY year ORDER BY revenue DESC) AS rank,\n",
    "\t\t   COUNT(*) OVER(PARTITION BY year) AS number_of_companies\n",
    "\t  FROM revenue_analytics\n",
    ") t\n",
    "WHERE (rank * 1.0  / number_of_companies) <= 0.10;\n",
    "\n",
    "\n",
    "-- #2 Return the names of companies that grew their YoY revenue by at least 5%, consecutively every year.\n",
    "\n",
    "WITH yoy_growth AS (\n",
    "\t-- Calculate YoY growth for each company by dividing the revenue of\n",
    "\t-- current year by last year. \n",
    "\tSELECT company, year,\n",
    "\t\t   revenue * 1.0 / revenue_last_year yoy\n",
    "\tFROM (\n",
    "\t\t-- Shift the last year's revenue to the current year snapshot\n",
    "\t\tSELECT company, revenue, year,\n",
    "\t\t\t   LAG(revenue, 1) OVER(PARTITION BY company ORDER BY year) AS revenue_last_year\n",
    "\t\tFROM revenue_analytics\n",
    "\t) t\n",
    "), \n",
    "growth_companies AS (\n",
    "\t-- For each company, count the number of consecutive years that achieved 5% growths and\n",
    "\t-- the number of years that the company existed. If the number_of_years - 1 == consecutive\n",
    "\t-- growths, then the company matches the criteria\n",
    "\tSELECT company,\n",
    "\t\t   SUM(CASE WHEN yoy >= 1.05 THEN 1 ELSE 0 END) AS consecutive_growths,\n",
    "\t\t   COUNT(*) AS number_of_years\n",
    "\tFROM yoy_growth\n",
    "\tGROUP BY 1\n",
    ")\n",
    "-- Return the list of companies that always achieved 5% growths in revenue.\n",
    "SELECT company\n",
    "FROM growth_companies\n",
    "WHERE consecutive_growths = number_of_years - 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d0bf6",
   "metadata": {},
   "source": [
    "# 8. Statistical correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* statistical_correlation\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE stats  (\n",
    "\tx1 INTEGER,\n",
    "\tx2 INTEGER\n",
    "); \n",
    "\n",
    "\n",
    "COPY stats\n",
    "FROM '<Path to product_sql folder>/product_sql/statistical_correlation/stats.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "\n",
    "/* \n",
    "\n",
    "Difficulty: Medium\n",
    "Duration: 15 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "Calculate the statistical correlation of x1 and x2\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "-- Correlation Coefficient Formula: COV(x1, x2) / [STD(x1) * STD(x2)]\n",
    "\n",
    "WITH Mean AS (\n",
    "\t-- Mean\n",
    "\tSELECT\n",
    "\t\tx1,\n",
    "\t\tx2,\n",
    "\t\tAVG(x1) OVER() AS mean_x1,\n",
    "\t\tAVG(x2) OVER() AS mean_x2\n",
    "\tFROM stats\n",
    "),\n",
    "Variance AS (\n",
    "\t-- Variance\n",
    "\tSELECT\n",
    "\t\tAVG(POWER(x1 - mean_x1, 2)) AS var_x1,\n",
    "\t\tAVG(POWER(x2 - mean_x2, 2)) AS var_x2\n",
    "\tFROM Mean\n",
    "), \n",
    "StandardDeviation AS (\n",
    "\t-- Standard deviation\n",
    "\tSELECT\n",
    "\t\tPOWER(var_x1, 0.5) AS std_x1,\n",
    "\t\tPOWER(var_x2, 0.5) AS std_x2\n",
    "\tFROM Variance\n",
    "),\n",
    "Covariance AS (\n",
    "\t-- Covariance\n",
    "\tSELECT \n",
    "\t\tAVG((x1 - mean_x1) * (x2 - mean_x2)) AS cov_x1_x2\n",
    "\tFROM Mean\n",
    ")\n",
    "-- Correlation Coefficient\n",
    "SELECT\n",
    "\tcov_x1_x2 / (std_x1 * std_x2) AS corr_x1_x2\n",
    "FROM\n",
    "\tStandardDeviation, Covariance;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b19d61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T05:00:03.439477Z",
     "iopub.status.busy": "2022-01-05T05:00:03.439076Z",
     "iopub.status.idle": "2022-01-05T05:00:03.447161Z",
     "shell.execute_reply": "2022-01-05T05:00:03.445363Z",
     "shell.execute_reply.started": "2022-01-05T05:00:03.439442Z"
    }
   },
   "source": [
    "# 9. twitch_content_violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Twitch Content Violations\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE twitch_content_violations (\n",
    "\tdate DATE,\n",
    "\tuser_id VARCHAR,\n",
    "\tvideo INTEGER,\n",
    "\tsexual INTEGER,\n",
    "\thateful INTEGER,\n",
    "\tspam INTEGER\n",
    "); \n",
    "\n",
    "\n",
    "COPY twitch_content_violations\n",
    "FROM '<Path to product_sql folder>/product_sql/twitch_content_violations/twitch_content_violations.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Hard\n",
    "Duration: 30 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "The integrity team in Twitch, a video streaming platform for games, ensures that publishers \n",
    "follow a community guidelines that video content is not sexual, hateful or spammy. \n",
    "Use the violations table below to address three part questions. Address these two questions:\n",
    "\n",
    "#1 - ​On a monthly basis, how many users publish at least one video that violates all \n",
    "three categories - sexual, hateful and spammy?\n",
    "\n",
    "#2 - ​Currently, the integrity team doesn’t enforce banning a user unless the number of \n",
    "violations exceeds ten. A revision is proposed such that a user is banned if the number of \n",
    "violations accumulated ​exceeds three​. For each user, return two records:\n",
    "\n",
    "\t1. The first record shows the date, user_id and status “0” when a user \n",
    "\tpublished a video for the first time.\n",
    "\t\n",
    "\t2. The last record shows the date, user_id and status “1” when a user \n",
    "\tpublished a video for the last time before being banned.\n",
    "\n",
    "For users who are not banned, only return the first record.\n",
    "*/\n",
    "\n",
    "\n",
    "-- #1 - On a monthly basis, how many users publish at least one video that violates all \n",
    "-- three categories - sexual, hateful and spammy?\n",
    "\n",
    "-- The top-level query applies group by on year, month and counts rows that represent \n",
    "-- instances when users published videos that violated three rules.\n",
    "SELECT year, month, COUNT(*)\n",
    "FROM (\n",
    "\t-- The sub-query first filters on videos that violate all categories then removes \n",
    "\t-- duplicate user_id's using DISTINCT on year,month,user_id\n",
    "\tSELECT DISTINCT\n",
    "\t\tEXTRACT(YEAR FROM date) AS year, \n",
    "\t    EXTRACT(MONTH FROM date) AS month, \n",
    "\t    user_id \n",
    "\tFROM violations\n",
    "\tWHERE (sexual + hateful + spam) = 3 \n",
    ") t\n",
    "GROUP BY year, month;\n",
    "\n",
    "\n",
    "-- #2 - Currently, the integrity team doesn’t enforce banning a user unless the number of violations exceeds ten. \n",
    "-- A revision is proposed such that a user is banned if the number of violations accumulated exceeds three. \n",
    "-- For each user, return two rows: the date of the first video and date when banned. Additionally, create an \n",
    "-- indicator column that shows status of whether a user signed up \"0\" or banned \"1.\" The query should return\n",
    "-- a table with user_id, status, date. Note that some users are not banned; hence, only one row is returned.\n",
    "\n",
    "\n",
    "-- The first WITH clause calculates the cumulative violations per user across dates.\n",
    "WITH cumulative_violation_table AS (\n",
    "SELECT user_id,\n",
    "               date,\n",
    "\tSUM(total_violations) OVER (PARTITION BY user_id ORDER BY date, video ASC) AS cumulative_violations\n",
    "FROM (\n",
    "SELECT user_id,\n",
    "\t   video,\n",
    "\t   date,\n",
    "\t   (sexual + hateful + spam) AS total_violations\n",
    "FROM violations ) t1\n",
    "),\n",
    "-- The second WITH clause takes the sub-query that creates the status indicator below and \n",
    "-- sets the key dates, first and last videos, based on the date of the first video in each \n",
    "-- status per user.\n",
    "status_minimum_date AS (\n",
    "SELECT user_id, \n",
    "\t   status, \n",
    "\t   MIN(date) as date\n",
    "FROM (\n",
    "    -- The subquery creates a status indicator if the user is banned \"1\" or not \"0\" on a particular day.\n",
    "\tSELECT user_id, \n",
    "\t \t   date,\t\n",
    "\t\t   CASE WHEN cumulative_violations <= 3 THEN 0 ELSE  1 END AS status\n",
    "FROM cumulative_violation_table ) t\n",
    "GROUP BY user_id, status \n",
    ")\n",
    "-- Final table that retrieves status and date per user.\n",
    "SELECT * FROM status_minimum_date ORDER BY user_id, status;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47b4da",
   "metadata": {},
   "source": [
    "# 10. Uber engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Uber Engagement\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE uber_engagement_users (\n",
    "\tuser_id VARCHAR,\n",
    "\trole VARCHAR,\n",
    "\tcity VARCHAR,\n",
    "\tbanned BOOLEAN \n",
    "); \n",
    "\n",
    "CREATE TABLE uber_engagement_trips (\n",
    "\tid VARCHAR,\n",
    "\tclient_id VARCHAR,\n",
    "\tdriver_id VARCHAR,\n",
    "\tstatus VARCHAR,\n",
    "\trequest_date DATE\n",
    "); \n",
    "\n",
    "COPY uber_engagement_users\n",
    "FROM '<Path to product_sql folder>/product_sql/uber_engagement/uber_engagement_users.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "COPY uber_engagement_trips\n",
    "FROM '<Path to product_sql folder>/product_sql/uber_engagement/uber_engagement_trips.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Hard\n",
    "Duration: 30 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "There are two tables - the uber_engagement_users table contains user_id, role (i.e. driver, \n",
    "client) and city (i.e. NYC, SF, Berlin, Tokyo) and banned (i.e. T, F), and the\n",
    "uber_engagement_trips table contains, id, client_id, driver_id, status (i.e. completed, \n",
    "cancelled_by_driver, cancelled_by_client), and request_date. \n",
    "\n",
    "Address the two questions:\n",
    "\n",
    "#1. Between August 01, 2021 and August 12, 2021, what percentage of requests made by unbanned \n",
    "\tclients each day were cancelled in each city? \n",
    "#2. Between August 01, 2021 and August 12, 2021, among SF and NYC, in each city, list top \n",
    "\tthree drivers by the number of completed trips.\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "-- Solution #1\n",
    "WITH UnbannedClients AS (\n",
    "\t-- This temp table filters on unbanned clients\n",
    "\tSELECT user_id, city\n",
    "\tFROM uber_engagement_users\n",
    "\tWHERE role = 'client'\n",
    "\t  AND banned = False\n",
    "),\n",
    "FilterTripsOnDates AS (\n",
    "\t-- This temp table filters the trip table given request dates\n",
    "\tSELECT \n",
    "\t\tclient_id AS user_id,\n",
    "\t\tstatus,\n",
    "\t\trequest_date\n",
    "\tFROM uber_engagement_trips\n",
    "\tWHERE request_date BETWEEN '2021-08-01' AND '2021-08-12'\n",
    "),\n",
    "TripsByUnbannedClients AS (\n",
    "\t-- Table joins the trips to the client table\n",
    "\tSELECT user_id, city, status, request_date\n",
    "\tFROM FilterTripsOnDates JOIN UnbannedClients\n",
    "\t\tUSING (user_id)\n",
    ")\n",
    "SELECT \n",
    "\t-- By city and date, average the number of instances where the ride status \n",
    "\t-- is not complete.\n",
    "\tcity,\n",
    "\trequest_date,\n",
    "\tAVG(CASE WHEN status != 'completed' THEN 1 ELSE 0 END) AS cancellation_rate \n",
    "FROM TripsByUnbannedClients\n",
    "GROUP BY city, request_date\n",
    "ORDER BY city, request_date;\n",
    "\n",
    "\n",
    "-- Solution #2\n",
    "WITH DriversInSelectCities AS (\n",
    "\t-- Filters on drivers in NYC and SF\n",
    "\tSELECT \n",
    "\t\tuser_id AS driver_id, \n",
    "\t\tcity\n",
    "\tFROM uber_engagement_users\n",
    "\tWHERE \n",
    "\t\tcity IN ('NYC', 'SF') AND\n",
    "\t\trole = 'driver'\n",
    "),\n",
    "DriverTrips AS (\n",
    "\t-- Filters on drivers who performed trips between dates provided.\n",
    "\tSELECT \n",
    "\t\tdriver_id,\n",
    "\t\tcity,\n",
    "\t\trequest_date,\n",
    "\t\tstatus\n",
    "\tFROM DriversInSelectCities\n",
    "\tJOIN uber_engagement_trips\n",
    "\t\tUSING (driver_id)\n",
    "\tWHERE request_date BETWEEN '2021-08-01' AND '2021-08-12'\n",
    "),\n",
    "CompletedTrips AS (\n",
    "\t-- Count the number of completed trips per driver in each city\n",
    "\tSELECT\n",
    "\t\tcity,\n",
    "\t\tdriver_id,\n",
    "\t\tCOUNT(*) AS trips_completed\n",
    "\tFROM DriverTrips \n",
    "\tWHERE status = 'completed'\n",
    "\tGROUP BY city, driver_id\n",
    "),\n",
    "RankDrivers AS (\n",
    "\t-- Apply dense_rank as multiple drivers could have completed the same number of trips.\n",
    "\tSELECT city,\n",
    "\t\ttrips_completed,\n",
    "\t\tDENSE_RANK() OVER(PARTITION BY city ORDER BY trips_completed) AS driver_rank\n",
    "\tFROM CompletedTrips\n",
    ")\n",
    "-- Filter on top three drivers.\n",
    "SELECT city, driver_rank, trips_completed\n",
    "FROM RankDrivers\n",
    "WHERE driver_rank <= 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6b87a",
   "metadata": {},
   "source": [
    "# 11. User Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* User Subscription\n",
    "\n",
    "Instruction:\n",
    "\n",
    "Please run the CREATE TABLE and COPY commands below that will create the data tables\n",
    "for this problem. Note that, in the COPY command, you will need to specify the directory \n",
    "where your data folder is located. \n",
    "\n",
    "For instance, if your SQL course folder is located in the home directory, then\n",
    "set the path as /Home/product_sql/find_median/user_creation.csv.\n",
    "\n",
    "Once your data tables are created, you are ready to solve the problem!\n",
    "\n",
    "*/\n",
    "\n",
    "CREATE TABLE user_subscription (\n",
    "\tuser_id VARCHAR,\n",
    "\ttimestamp INTEGER,\n",
    "\taction VARCHAR \n",
    "); \n",
    "\n",
    "COPY user_subscription\n",
    "FROM '<Path to product_sql folder>/product_sql/user_subscription/user_subscription.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;\n",
    "\n",
    "/*\n",
    "\n",
    "Difficulty: Medium\n",
    "Duration: 15 Minutes\n",
    "\n",
    "Problem:\n",
    "\n",
    "There is a table called user_action which contains user_id, timestamp and action (visit, subscribe, \n",
    "and cancel). A user can visit, subcribe, cancel multiple times. An example could be:\n",
    "\n",
    "User A: Visit, Visit \n",
    "User B: Visit, Visit, Subscribe\n",
    "User C: Visit, Visit, Subscribe, Cancel, Subscribe\n",
    "\n",
    "Address the two questions:\n",
    "\n",
    "#1. How long did it take for each user to subscribe after the first page visit?\n",
    "#2. How many users cancelled then re-subscribed again? Only count the users who \n",
    "have remained subscribed after re-subscribing.\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "-- Solution #1\n",
    "WITH Visit AS (\n",
    "\t-- This temp table filters on users's first visit using row_number()\n",
    "\tSELECT user_id, timestamp\n",
    "\tFROM (\t\n",
    "\t\tSELECT user_id, timestamp, \n",
    "\t\t\trow_number() OVER(PARTITION BY user_id ORDER BY timestamp ASC) as row_number\n",
    "\t\tFROM user_subscription\t\t\t\n",
    "\t\tWHERE action = 'visit'\n",
    "\t) subquery \n",
    "\tWHERE row_number = 1\n",
    "),\n",
    "Subscribe AS (\n",
    "\t-- This temp table filters on users's first subscription using row_number() \n",
    "\t-- Note that a user could cancel and re-subscribe. So, it's important to \n",
    "\t-- filter on the first subscription record. \n",
    "\tSELECT user_id, timestamp\n",
    "\tFROM (\t\n",
    "\t\tSELECT user_id, timestamp,\n",
    "\t\t\trow_number() OVER(PARTITION BY user_id ORDER BY timestamp ASC) as row_number\n",
    "\t\tFROM user_subscription\n",
    "\t\tWHERE action = 'subscribe'\t\n",
    "\t) subquery\n",
    "\tWHERE row_number = 1\n",
    ")\n",
    "-- Compute the difference in the timestamps between the first subscribe and first visit\n",
    "-- timestamps.\n",
    "SELECT Visit.user_id,\n",
    "\t   Subscribe.timestamp - Visit.timestamp AS delta\n",
    "FROM visit JOIN Subscribe USING (user_id);\n",
    "\n",
    "\n",
    "-- Solution #2\n",
    "WITH LatestCancel AS (\n",
    "\t-- This temp table filters on users's last cancelled using row_number(). Similar to \n",
    "\t-- subscription, a user can subscribe, cancel, re-subscribe, and re-cancel. So, it's\n",
    "\t-- important to select the last instance of churn using ORDER BY timestamp DESC.\n",
    "\tSELECT user_id, timestamp\n",
    "\tFROM (\t\n",
    "\t\tSELECT user_id, timestamp,\n",
    "\t\t\trow_number() OVER(PARTITION BY user_id ORDER BY timestamp DESC) as row_number\n",
    "\t\tFROM user_subscription\n",
    "\t\tWHERE action = 'churn'\t\n",
    "\t) subquery\n",
    "\tWHERE row_number = 1\n",
    "),\n",
    "LatestSubscribe AS (\n",
    "\t-- This temp table filters on users's last re-subscription using row_number(). Similar to \n",
    "\t-- subscription, a user can subscribe, cancel, re-subscribe, and re-cancel. So, it's\n",
    "\t-- important to select the last instance of subscription using ORDER BY timestamp DESC.\n",
    "\tSELECT user_id, timestamp\n",
    "\tFROM (\t\n",
    "\t\tSELECT user_id, timestamp,\n",
    "\t\t\trow_number() OVER(PARTITION BY user_id ORDER BY timestamp DESC) as row_number\n",
    "\t\tFROM user_subscription\n",
    "\t\tWHERE action = 'subscribe'\t\n",
    "\t) subquery\n",
    "\tWHERE row_number = 1\n",
    ")\n",
    "-- JOIN LatestCancel and LatestSubscribe then filter on instances where the\n",
    "-- timestamp in LatestSubscribe is greater than timestamp in LatestCancel.\n",
    "-- Finally, count the records which returns the number of unique instances\n",
    "-- where users re-subscribed and remained subscribed thus far.\n",
    "SELECT COUNT(*)\n",
    "FROM LatestCancel JOIN LatestSubscribe USING (user_id)\n",
    "WHERE LatestSubscribe.timestamp > LatestCancel.timestamp;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986eab9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-2.4-default",
   "language": "python",
   "name": "py3-spark-2.4-kernel-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
